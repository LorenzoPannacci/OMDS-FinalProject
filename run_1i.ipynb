{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "f7Wgq9EwxtLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "APOK8nWfe8Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import minimize"
      ],
      "metadata": {
        "id": "y0-JKt16ylFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "qYz0z7vCe_Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_q1():\n",
        "    '''\n",
        "    This function downloads and prepares the data for the question 1.\n",
        "\n",
        "    Since the data was originally uploaded on the Classroom page of the course\n",
        "    and Colab is not able to access it, the .csv file has been re-uploaded\n",
        "    unaltered to the Google Drive of my student account and made public for\n",
        "    Colab to access.\n",
        "\n",
        "    For the train-test split it has been choosen the classic 80/20 split and the\n",
        "    random state has been fixed to ensure reproducibility of the results.\n",
        "    '''\n",
        "\n",
        "    # download age prediction data (flag -q used to avoid prints)\n",
        "    !gdown 1mOTd6ZjyILyRPqhqi3dgXWSKBwBiNvdu -q\n",
        "\n",
        "    # load data into Pandas\n",
        "    df = pd.read_csv('/content/AGE_PREDICTION.csv')\n",
        "\n",
        "    # convert data to numpy data structures\n",
        "    x = df.iloc[:, 0 : 32].to_numpy()\n",
        "    y = df.iloc[:, 32].to_numpy().reshape(-1, 1)\n",
        "\n",
        "    # split data into training and test sets\n",
        "    # validation is not split here as we will use k-fold cross-validation\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    # normalize data\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "lLGgZ_fsfHyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary functions"
      ],
      "metadata": {
        "id": "WK2JWNekfIMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Losses and errors"
      ],
      "metadata": {
        "id": "Pjs5DRgJHXbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def non_regularized_loss(y_pred, y_true):\n",
        "    '''\n",
        "    This function implements the non-regularized loss function that is\n",
        "    required for the report. It is just a mean square error loss and implements\n",
        "    the following formula:\n",
        "\n",
        "    MSE = 1 / N * \\sum_{i = 1}^{N} (y_i - y_i\\hat)^2\n",
        "    '''\n",
        "\n",
        "    return (1.0 / len(y_true)) * np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "def l2_loss(y_pred, y_true, lam, weights):\n",
        "    '''\n",
        "    This function implements the L2 loss function. This is the main loss\n",
        "    of the neural network, our objective function. It is the sum of the mean\n",
        "    square error loss and the L2 regularization term, which is defined as:\n",
        "\n",
        "    \\lam * \\sum_{i = 1}^{L} \\sum_{j = 1}^{N_i} w_{ij}^2\n",
        "    '''\n",
        "\n",
        "    # mean square error component of the loss\n",
        "    mse = (1.0 / len(y_true)) * np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "    # regularization component of the loss\n",
        "    reg = lam * np.sum([np.sum(omega ** 2) for omega in weights])\n",
        "\n",
        "    return mse + reg\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    '''\n",
        "    This function implements the Mean Absolute Percentage Error. This is the\n",
        "    metric used to evaluate the performances of the neural network. It\n",
        "    implements the formula:\n",
        "\n",
        "    MAPE = 100 / N * \\sum_{i = 1}^{N} \\frac{|y_i - y_i\\hat|}{y_i}\n",
        "    '''\n",
        "\n",
        "    return (100 / len(y_true)) * np.sum(np.abs((y_true - y_pred) / y_true))"
      ],
      "metadata": {
        "id": "uipZIfXmHggt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation functions and derivatives"
      ],
      "metadata": {
        "id": "GC4SWIorHaEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    '''\n",
        "    Sigmoid function. We clip the input to avoid overflow of the exponential.\n",
        "    '''\n",
        "\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_der(x):\n",
        "    '''\n",
        "    Derivative of the sigmoid function.\n",
        "    '''\n",
        "\n",
        "    sig = sigmoid(x)\n",
        "\n",
        "    return sig * (1.0 - sig)\n",
        "\n",
        "def tanh(x):\n",
        "    '''\n",
        "    Hyperbolic tangent function.\n",
        "    '''\n",
        "\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_der(x):\n",
        "    '''\n",
        "    Derivative of the hyperbolic tangent function.\n",
        "    '''\n",
        "\n",
        "    tanh = np.tanh(x)\n",
        "\n",
        "    return 1.0 - np.square(tanh)\n",
        "\n",
        "def swish(x):\n",
        "    '''\n",
        "    1-Swish function.\n",
        "    '''\n",
        "\n",
        "    return x * sigmoid(x)\n",
        "\n",
        "def swish_der(x):\n",
        "    '''\n",
        "    Derivative of the 1-Swish function.\n",
        "    '''\n",
        "\n",
        "    sig = sigmoid(x)\n",
        "\n",
        "    return sig + x * sig * (1 - sig)"
      ],
      "metadata": {
        "id": "1WgPtkUsHi_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "IZiHl6q7HcCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_normal_init(in_dim, out_dim):\n",
        "    '''\n",
        "    This function implements the Xavier normal initialization for the weights.\n",
        "    '''\n",
        "\n",
        "    # standard deviation of xavier normal initialization\n",
        "    sigma = np.sqrt(2.0 / (in_dim + out_dim))\n",
        "\n",
        "    return np.random.randn(in_dim, out_dim) * sigma"
      ],
      "metadata": {
        "id": "vPtPlXPNHdqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "JQKWi3CdfnsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "d7jpKMObfU1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP():\n",
        "    '''\n",
        "    This class implements a Multi-Layer Perceptron neural network for the\n",
        "    regression task. The neural network is implemented from scratch from numpy\n",
        "    structures. In particular the forward pass and backpropagation are\n",
        "    implemented following the formulas presented in the report.\n",
        "\n",
        "    The training of the neural network is done using scipy.optimize.minimize()\n",
        "    method.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, L, N, lam, activation_name, input_size=32):\n",
        "        '''\n",
        "        This function initializes the Multi-Layer Perceptron object. It sets\n",
        "        the parameters of the neural network and initializes the weights and\n",
        "        biases.\n",
        "\n",
        "        The weights are initialized using the Xavier's initialization.\n",
        "        The biases are initialized to zeros.\n",
        "\n",
        "        The input size is defaulted to 32 since it is the number of features of\n",
        "        the provided dataset.\n",
        "        '''\n",
        "\n",
        "        # object params initialization\n",
        "        self.L = L\n",
        "        self.N = N\n",
        "        self.lam = lam\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # we get the activation function and its derivative from the names of\n",
        "        # the functions which are defined in cells above\n",
        "        self.activation = globals().get(activation_name)\n",
        "        self.activation_der = globals().get(activation_name + \"_der\")\n",
        "\n",
        "        # initialize lists of weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # weights and biases initialization:\n",
        "        # we will use xavier's initialization for the weights\n",
        "        # and an zeros for the biases\n",
        "\n",
        "        # from input layer to first hidden layer\n",
        "        self.weights.append(xavier_normal_init(self.input_size, N[0]))\n",
        "        self.biases.append(np.zeros(N[0]))\n",
        "\n",
        "        # between hidden layers\n",
        "        for i in range(1, L):\n",
        "            self.weights.append(xavier_normal_init(N[i - 1], N[i]))\n",
        "            self.biases.append(np.zeros(N[i]))\n",
        "\n",
        "        # from last hidden layer to output layer\n",
        "        self.weights.append(xavier_normal_init(N[-1], 1))\n",
        "        self.biases.append(np.zeros(1))\n",
        "\n",
        "    def convert_params(self, weights = None, biases = None):\n",
        "        '''\n",
        "        This function transforms the weights matrices and biases vectors into a\n",
        "        single vector. This is needed because the scipy.optimize.minimize()\n",
        "        function requires a vector as input of the function to optimize.\n",
        "\n",
        "        We will therefore need to convert back and forth between those two\n",
        "        representations during training\n",
        "        '''\n",
        "\n",
        "        # if we pass no parameters we use the values of the object\n",
        "        if weights is None and biases is None:\n",
        "            weights = self.weights\n",
        "            biases = self.biases\n",
        "\n",
        "        # convert all weights to single flat vector\n",
        "        flat_weights = np.concatenate([w.flatten() for w in weights])\n",
        "\n",
        "        # convert all biases to single flat vector\n",
        "        flat_biases = np.concatenate([b.flatten() for b in biases])\n",
        "\n",
        "        # merge the two vectors and return the complete parameters vector\n",
        "        return np.concatenate([flat_weights, flat_biases])\n",
        "\n",
        "    def update_params(self, new_params):\n",
        "        '''\n",
        "        This function takes as input the vector of all weights and biases\n",
        "        provided by the scipy.optimize.minimize() function and transforms it\n",
        "        into the weights matrices and biases vectors of the MLP object.\n",
        "\n",
        "        It exploits the fact that the positions of weights and biases are\n",
        "        fixed inside the new_params vector to take chunks of the vector and\n",
        "        reshape them in the correct forms.\n",
        "        '''\n",
        "\n",
        "        offset = 0\n",
        "\n",
        "        # from input layer to first hidden layers\n",
        "        self.weights[0] = new_params[offset : offset + self.input_size * self.N[0]].reshape(self.input_size, self.N[0])\n",
        "        offset += self.input_size * self.N[0]\n",
        "\n",
        "        # between hidden layers\n",
        "        for i in range(1, self.L):\n",
        "            self.weights[i] = new_params[offset : offset + self.N[i - 1] * self.N[i]].reshape(self.N[i - 1], self.N[i])\n",
        "            offset += self.N[i - 1] * self.N[i]\n",
        "\n",
        "        # from last hidden layer to output layer\n",
        "        self.weights[self.L] = new_params[offset : offset + self.N[-1]].reshape(self.N[-1], 1)\n",
        "        offset += self.N[-1]\n",
        "\n",
        "        # from input layer to first hidden layers\n",
        "        self.biases[0] = new_params[offset : offset + self.N[0]]\n",
        "        offset += self.N[0]\n",
        "\n",
        "        # between hidden layers\n",
        "        for i in range(1, self.L):\n",
        "            self.biases[i] = new_params[offset : offset + self.N[i]]\n",
        "            offset += self.N[i]\n",
        "\n",
        "        # from input layer to first hidden layers\n",
        "        self.biases[-1] = new_params[offset : offset + 1]\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        '''\n",
        "        This function computes the predictions and then the loss between the\n",
        "        ground truth and the predicted values.\n",
        "        '''\n",
        "\n",
        "        y_pred = self.predict(x)\n",
        "\n",
        "        return l2_loss(y_pred, y, self.lam, self.weights)\n",
        "\n",
        "    def get_non_regularized_loss(self, x, y):\n",
        "        '''\n",
        "        This function is the same as the one above but uses the non-regularized\n",
        "        loss as printing its value is a requirement of the homework.\n",
        "        '''\n",
        "\n",
        "        y_pred = self.predict(x)\n",
        "\n",
        "        return non_regularized_loss(y_pred, y)\n",
        "\n",
        "    def objective(self, new_params, x, y):\n",
        "        '''\n",
        "        This function converts the loss into a function that can be used by the\n",
        "        scipy.optimize.minimize() method as the 'fun' callable, the objective\n",
        "        function to be minimized.\n",
        "\n",
        "        The first argument is the vector of values the minimizer is optimizing\n",
        "        on, the vector of weights and biases. We first update the weights and\n",
        "        biases and then compute the loss on the train set.\n",
        "        '''\n",
        "\n",
        "        self.update_params(new_params)\n",
        "\n",
        "        return self.get_loss(x, y)\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        '''\n",
        "        This function implements the forward pass of the neural network.\n",
        "\n",
        "        The formulas used here are derived and justified in the report.\n",
        "\n",
        "        Following the structure of the neural network we apply dot products\n",
        "        with weight matrices and activations functions and then sum the bias\n",
        "        until we arrive to the output layer. What we obtain at the end is the\n",
        "        prediction of the neural network.\n",
        "\n",
        "        We memorize the values at each step since they are needed for the\n",
        "        backpropagation.\n",
        "\n",
        "        We call Z what we obtain after a dot product and we call A what we\n",
        "        obtain after applying the activation function, following the same\n",
        "        nomenclature used in the report.\n",
        "        '''\n",
        "\n",
        "        # preallocate memory for Z and A\n",
        "        Z = np.array([None] * self.L)\n",
        "        A = np.array([None] * self.L)\n",
        "\n",
        "        # input layer\n",
        "        Z[0] = np.dot(x, self.weights[0]) + self.biases[0]\n",
        "        A[0] = self.activation(Z[0])\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(1, self.L):\n",
        "            Z[i] = np.dot(A[i - 1], self.weights[i]) + self.biases[i]\n",
        "            A[i] = self.activation(Z[i])\n",
        "\n",
        "        # output layer\n",
        "        y_pred = np.dot(A[self.L - 1], self.weights[self.L]) + self.biases[self.L]\n",
        "\n",
        "        return Z, A, y_pred\n",
        "\n",
        "    def backpropagation(self, x, y_true):\n",
        "        '''\n",
        "        This function implements the backpropagation of the neural network.\n",
        "\n",
        "        The formulas used here are derived and justified in the report.\n",
        "\n",
        "        The fact that we are using an L2-regularized loss instead of just the\n",
        "        MSE means that when we are computing the derivative of the error with\n",
        "        respect to the weights we need to add the derivative of the\n",
        "        regularization term, that for L2 are simply the weights themselves.\n",
        "\n",
        "        For the other components the backpropagation behaves the same as a\n",
        "        non-regularized MSE loss.\n",
        "\n",
        "        We call Z what we obtain after a dot product and we call A what we\n",
        "        obtain after applying the activation function, following the same\n",
        "        nomenclature used in the report.\n",
        "        '''\n",
        "\n",
        "        # backpropagate through all layers and compute the gradients\n",
        "\n",
        "        n_samples = x.shape[0]\n",
        "        Z, A, y_pred = self.forward_pass(x)\n",
        "\n",
        "        # preallocate memory for the gradients\n",
        "        dEdW = np.array([None] * len(self.weights))\n",
        "        dEdB = np.array([None] * len(self.biases))\n",
        "\n",
        "        # OUTPUT LAYER\n",
        "\n",
        "        dEdY = - 2 * (y_true - y_pred) # = dEdAL = dEdZL since no activation function in the last layer\n",
        "\n",
        "        dEdW[-1] = np.dot(A[-1].T, dEdY) / n_samples + 2 * self.lam * self.weights[-1]\n",
        "        dEdB[-1] = np.sum(dEdY) / n_samples\n",
        "\n",
        "        # HIDDEN LAYERS\n",
        "\n",
        "        dEdA = np.dot(dEdY, self.weights[-1].T)\n",
        "\n",
        "        for i in reversed(range(1, self.L)):\n",
        "            dEdZ = dEdA * self.activation_der(Z[i])\n",
        "\n",
        "            dEdW[i] = np.dot(A[i - 1].T, dEdZ) / n_samples + 2 * self.lam * self.weights[i]\n",
        "\n",
        "            dEdB[i] = np.sum(dEdZ, axis = 0) / n_samples\n",
        "\n",
        "            dEdA = np.dot(dEdZ, self.weights[i].T)\n",
        "\n",
        "        # INPUT LAYER\n",
        "\n",
        "        dEdZ = dEdA * self.activation_der(Z[0])\n",
        "\n",
        "        dEdW[0] = np.dot(x.T, dEdZ) / n_samples + 2 * self.lam * self.weights[0]\n",
        "        dEdB[0] = np.sum(dEdZ, axis=0) / n_samples\n",
        "\n",
        "        return dEdW, dEdB\n",
        "\n",
        "    def backpropagation_opt(self, new_params, x, y):\n",
        "        '''\n",
        "        This function converts the backpropagation operation into a function\n",
        "        that can be used by the scipy.optimize.minimize() method as the 'jac'\n",
        "        callable, the method for computing the gradient vector.\n",
        "\n",
        "        As for the objective the first argument is the vector of values the\n",
        "        minimizer is optimizing on, the vector of weights and biases. We first\n",
        "        update the weights and biases and then compute the backpropagation.\n",
        "\n",
        "        After the backpropagation we convert the gradients into the format\n",
        "        required by the scipy.optimize.minimize().\n",
        "        '''\n",
        "\n",
        "        self.update_params(new_params)\n",
        "\n",
        "        dEdW, dEdB = self.backpropagation(x, y)\n",
        "\n",
        "        return self.convert_params(dEdW, dEdB)\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        This function predicts the output of the neural network for the given\n",
        "        input. It executes the forward pass but keeps only the last element,\n",
        "        which is the prediction.\n",
        "        '''\n",
        "\n",
        "        return self.forward_pass(x)[-1]\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        '''\n",
        "        This function evaluates the MAPE  on given input data.\n",
        "        '''\n",
        "\n",
        "        y_pred = self.predict(x)\n",
        "\n",
        "        return mape(y, y_pred)\n",
        "\n",
        "    def train(self, x, y, verbose = False):\n",
        "        '''\n",
        "        This function implements the training of the neural network using the\n",
        "        scipy.optimize.minimize() method. Optimization time is measured as it\n",
        "        is required by the homework.\n",
        "        '''\n",
        "\n",
        "        # start measuring time\n",
        "        time_init = time.process_time()\n",
        "\n",
        "        # solve optimization problem\n",
        "        result = minimize(fun = self.objective,\n",
        "                          x0 = self.convert_params(),\n",
        "                          jac = self.backpropagation_opt,\n",
        "                          args = (x, y),\n",
        "                          method = 'L-BFGS-B',\n",
        "                          options = {\"maxiter\" : 1000,\n",
        "                                     \"maxls\" : 20,\n",
        "                                     \"ftol\" : 1e-4})\n",
        "\n",
        "        # stop measuring time and compute CPU time of optimization\n",
        "        time_end = time.process_time()\n",
        "        time_delta = time_end - time_init\n",
        "\n",
        "        # update weights and biases to optimal value found\n",
        "        self.update_params(result.x)\n",
        "\n",
        "        if verbose:\n",
        "            return result, time_delta\n",
        "        else:\n",
        "            return result"
      ],
      "metadata": {
        "id": "VtgVogC6CiZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-validation"
      ],
      "metadata": {
        "id": "8Jzh_WDOfqdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_cross_validation(network_structures, lambdas, activations, k, show_progress = True):\n",
        "    '''\n",
        "    This function implements the cross-validation for the MLP hyperparameters.\n",
        "    The hyperparameters we work on are:\n",
        "    - The number of layers\n",
        "    - The number of neurons per layer\n",
        "    - The activation function\n",
        "    - The regularization parameter lambda\n",
        "\n",
        "    Search space is inputted to the method.\n",
        "    '''\n",
        "\n",
        "    # load data question 1\n",
        "    x, _, y, _ = prepare_data_q1()\n",
        "\n",
        "    # generate all possible combinations\n",
        "    combinations = list(itertools.product(network_structures, lambdas, activations))\n",
        "\n",
        "    # split train data into folds\n",
        "    k_folds = KFold(n_splits = k)\n",
        "\n",
        "    # initialize variables to find best combination\n",
        "    best_val_MAPE = float('inf')\n",
        "    best_train_MAPE = None\n",
        "    best_val_error = None\n",
        "    best_train_error = None\n",
        "    best_combination = None\n",
        "\n",
        "    for combination in tqdm(combinations) if show_progress else combinations:\n",
        "        fold_val_MAPE = []\n",
        "        fold_train_MAPE = []\n",
        "        fold_val_error = []\n",
        "        fold_train_error = []\n",
        "\n",
        "        for train_index, val_index in k_folds.split(x):\n",
        "            # split data into train and validation sets\n",
        "            x_train = x[train_index]\n",
        "            y_train = y[train_index]\n",
        "\n",
        "            x_val = x[val_index]\n",
        "            y_val = y[val_index]\n",
        "\n",
        "            # initialize and train model\n",
        "            np.random.seed(42)\n",
        "            model = MLP(L = len(combination[0]),\n",
        "                        N = combination[0],\n",
        "                        lam = combination[1],\n",
        "                        activation_name = combination[2])\n",
        "            model.train(x_train, y_train)\n",
        "\n",
        "            # evaluate model\n",
        "            fold_val_MAPE.append(model.evaluate(x_val, y_val))\n",
        "            fold_train_MAPE.append(model.evaluate(x_train, y_train))\n",
        "            fold_val_error.append(model.get_loss(x_val, y_val))\n",
        "            fold_train_error.append(model.get_loss(x_train, y_train))\n",
        "\n",
        "        # compute combination MAPE as the mean of the MAPE for all folds\n",
        "        mean_val_MAPE = np.mean(fold_val_MAPE)\n",
        "        mean_train_MAPE = np.mean(fold_train_MAPE)\n",
        "        mean_val_error = np.mean(fold_val_error)\n",
        "        mean_train_error = np.mean(fold_train_error)\n",
        "\n",
        "        # update best combination if current combination is better\n",
        "        if mean_val_MAPE < best_val_MAPE:\n",
        "            best_val_MAPE = mean_val_MAPE\n",
        "            best_train_MAPE = mean_train_MAPE\n",
        "            best_val_error = mean_val_error\n",
        "            best_train_error = mean_train_error\n",
        "            best_combination = combination\n",
        "\n",
        "    return best_combination, best_val_MAPE, best_train_MAPE, best_val_error, best_train_error"
      ],
      "metadata": {
        "id": "aYQB3sdHfksi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required prints and values to compile the report"
      ],
      "metadata": {
        "id": "qr-YFVRugGc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_print(L, N, lam, activation):\n",
        "    '''\n",
        "    This function implements the training of the MLP and prints all the\n",
        "    informations required by the homework text.\n",
        "    '''\n",
        "\n",
        "    # load data question 1\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q1()\n",
        "\n",
        "    # initialize and train model\n",
        "    np.random.seed(42)\n",
        "    model = MLP(L, N, lam, activation)\n",
        "    result, time_delta = model.train(x_train, y_train, verbose = True)\n",
        "\n",
        "    # prints\n",
        "    print(f\"1. Number of layers L chosen: {L}\")\n",
        "    print(f\"2. Number of neurons N chosen: {N}\")\n",
        "    print(f\"3. Value of λ chosen: {lam}\")\n",
        "    print(f\"4. Values of other hyperparameters (activation function): {activation}\")\n",
        "    print(f\"5. Optimization solver chosen: L-BFGS-B\")\n",
        "    print(f\"6. Number of iterations: {result.nit}\")\n",
        "    print(f\"7. Time for optimizing the network (from when the solver is called until it stops): {time_delta} seconds\")\n",
        "    print(f\"8. Training Error (defined as in question (1), without regularization term): {model.get_non_regularized_loss(x_train, y_train)}\")\n",
        "    print(f\"9. Test Error (defined as above but on the test set): {model.get_non_regularized_loss(x_test, y_test)}\")"
      ],
      "metadata": {
        "id": "qIRu36nhiFNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_report(L, N, lam, activation):\n",
        "    '''\n",
        "    This function implements the training of the MLP and prints all the\n",
        "    informations to be inserted into the report.\n",
        "    '''\n",
        "\n",
        "    # The final setting for selected non-linearity, L, Nl, and λ\n",
        "    print(f\"Final setting for selected non-linearity, L, Nl, and λ:\")\n",
        "    print(f\"\\tNon-linearity: {activation}\")\n",
        "    print(f\"\\tL: {L}\")\n",
        "    print(f\"\\tNl: {N}\")\n",
        "    print(f\"\\tλ: {lam}\")\n",
        "\n",
        "    # which optimization routine you used for solving the minimization problem,\n",
        "    # the setting of its parameters (max number of iterations, etc.) and the\n",
        "    # returned message in output (successful optimization or others, number of\n",
        "    # iterations, starting/final value of the objective function) if any;\n",
        "    print(f\"Optimization routine used: scipy.optimize.minimize() with the solver L-BFGS-B\")\n",
        "    print(f\"Setting of its parameters:\")\n",
        "    print(f\"\\tMax number of iterations: 1000\")\n",
        "    print(f\"\\tMax number of iterations for linesearch: 20\")\n",
        "    print(f\"\\tTolerance on the objective function: 1e-4\")\n",
        "\n",
        "    # load data question 1\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q1()\n",
        "\n",
        "    # initialize model\n",
        "    np.random.seed(42)\n",
        "    model = MLP(L, N, lam, activation)\n",
        "    initial_error = model.get_loss(x_train, y_train)\n",
        "    initial_MAPE = model.evaluate(x_train, y_train)\n",
        "\n",
        "    # train model\n",
        "    result, time_delta = model.train(x_train, y_train, verbose = True)\n",
        "\n",
        "    print(f\"Output of the optimizer:\\n{result}\")\n",
        "\n",
        "    print(f\"Initial value of the regularized error on the training set: {initial_error}\")\n",
        "\n",
        "    print(f\"Final value of the regularized error on the training set: {model.get_loss(x_train, y_train)}\")\n",
        "\n",
        "    # the value of the error on the validation set (avg of errors on the k-folds)\n",
        "    _, kfold_val_MAPE, kfold_train_MAPE, kfold_val_error, kfold_train_error = MLP_cross_validation([N], [lam], [activation], k = 5, show_progress = False)\n",
        "    print(f\"Final value of the regularized error on the training set (k-folds): {kfold_train_error}\")\n",
        "    print(f\"Final value of the regularized error on the validation set: {kfold_val_error}\")\n",
        "\n",
        "    # the final value of the test error\n",
        "    print(f\"Final value of the test error: {model.get_loss(x_test, y_test)}\")\n",
        "\n",
        "    # the initial and final MAPE (average of MAPE values obtained with k-fold)\n",
        "    # on the training set\n",
        "    print(f\"Initial MAPE on training set: {initial_MAPE}\")\n",
        "    print(f\"Final MAPE on training set: {kfold_train_MAPE}\")\n",
        "\n",
        "\n",
        "    # the final MAPE on validation and test set\n",
        "    print(f\"Final MAPE on validation set: {kfold_val_MAPE}\")\n",
        "    print(f\"Final MAPE on test set: {model.evaluate(x_test, y_test)}\")\n",
        "\n",
        "    # TABLE\n",
        "\n",
        "    # optimization time\n",
        "    print(f\"Optimization time: {time_delta} seconds\")"
      ],
      "metadata": {
        "id": "lKwaldrInblR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "8T-UGkjaz051"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set mode to hide all non-required prints\n",
        "hide_non_required_prints = True"
      ],
      "metadata": {
        "id": "9wnrJliqJMRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup cross-validation space\n",
        "network_structures = [(8, 4), (16, 8), (32, 16), (64, 32), (16, 8, 4), (32, 16, 8), (64, 32, 16), (16, 8, 4, 2), (32, 16, 8, 4)]\n",
        "lambdas = [0, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1e+0]\n",
        "activations = [\"sigmoid\", \"tanh\", \"swish\"]\n",
        "\n",
        "# execute cross-validation\n",
        "best_combination, best_val_MAPE, best_train_MAPE, best_val_error, best_train_error = MLP_cross_validation(network_structures, lambdas, activations, k = 5)\n",
        "\n",
        "print(f\"\\nBest hyperparameters: {best_combination}\")\n",
        "# expected result is: ((64, 32), 0.05, 'swish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9EgxbUPsaSc",
        "outputId": "0b3e0630-d452-4183-ece9-646678660ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 270/270 [3:48:28<00:00, 50.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best hyperparameters: ((64, 32), 0.05, 'swish')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print values that are needed to compile the report but are not required\n",
        "# to be printed on the notebook\n",
        "\n",
        "# non-required and hidden\n",
        "if not hide_non_required_prints:\n",
        "    MLP_report(L = 2,\n",
        "               N = (64, 32),\n",
        "               lam = 0.05,\n",
        "               activation = 'swish')"
      ],
      "metadata": {
        "id": "Ro3USZkQJhts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print values that are required to be printed on the notebook\n",
        "\n",
        "MLP_print(L = 2,\n",
        "          N = (64, 32),\n",
        "          lam = 0.05,\n",
        "          activation = 'swish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuliEfEYKCXu",
        "outputId": "a08ef17f-10d6-4c34-faec-6c72ba2257a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Number of layers L chosen: 2\n",
            "2. Number of neurons N chosen: (64, 32)\n",
            "3. Value of λ chosen: 0.05\n",
            "4. Values of other hyperparameters (activation function): swish\n",
            "5. Optimization solver chosen: L-BFGS-B\n",
            "6. Number of iterations: 75\n",
            "7. Time for optimizing the network (from when the solver is called until it stops): 34.243861524999375 seconds\n",
            "8. Training Error (defined as in question (1), without regularization term): 96.27197172136378\n",
            "9. Test Error (defined as above but on the test set): 90.89298342505549\n"
          ]
        }
      ]
    }
  ]
}