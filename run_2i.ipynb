{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBw8tLkuBlsN"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR1mYK6ygLwL"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gj3ER0QwBlsU"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from functools import reduce\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from cvxopt import matrix, solvers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQW9DbN0hSju"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bqa6KJMshVCZ"
      },
      "outputs": [],
      "source": [
        "def prepare_data_q23():\n",
        "    '''\n",
        "    This function downloads and prepare the data for the question 2 and 3.\n",
        "\n",
        "    Since the data was originally uploaded on the Classroom page of the course\n",
        "    and Colab is not able to access it, the .csv file has been re-uploaded\n",
        "    unaltered to the Google Drive of my student account and made public for\n",
        "    Colab to access.\n",
        "\n",
        "    For the train-test split it has been choosen the classic 80/20 split and the\n",
        "    random state has been fixed to ensure reproducibility of the results even\n",
        "    if the funciton is called at different times throughout the notebook.\n",
        "    '''\n",
        "\n",
        "    # download gender classification data\n",
        "    !gdown 19v7WEWtHEpgTDr7PZwNGQNWa5u6tB0Cb -q\n",
        "\n",
        "    # load data into Pandas\n",
        "    df = pd.read_csv('/content/GENDER_CLASSIFICATION.csv')\n",
        "\n",
        "    # convert data to NumPy data structures\n",
        "    x = df.iloc[:, 0:32].to_numpy()\n",
        "    y = df.iloc[:, 32].to_numpy().reshape(-1, 1)\n",
        "\n",
        "    # converts classes of 1 and 0 to classes of 1 and -1\n",
        "    # which is required for SVM classification\n",
        "    y = 2 * y - 1\n",
        "\n",
        "    # split data into training and test sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    # normalize data\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "def prepare_data_q4():\n",
        "    '''\n",
        "    This function downloads and prepare the data for the question 4.\n",
        "\n",
        "    Since the data was originally uploaded on the Classroom page of the course\n",
        "    and Colab is not able to access it, the .csv file has been re-uploaded\n",
        "    unaltered to the Google Drive of my student account and made public for\n",
        "    Colab to access.\n",
        "\n",
        "    For the train-test split it has been choosen the classic 80/20 split and the\n",
        "    random state has been fixed to ensure reproducibility of the results.\n",
        "\n",
        "    The 3 classes selected out of 5 for the task are the first 3.\n",
        "    '''\n",
        "\n",
        "    # download ethnicity classification data\n",
        "    !gdown 1M_h_L7TM7-biHezB3jbHC5zAnK5ax9SP -q\n",
        "\n",
        "    # load data into Pandas\n",
        "    df = pd.read_csv('/content/ETHNICITY_CLASSIFICATION.csv')\n",
        "\n",
        "    # select only three classes are requested by the question\n",
        "    df = df[df['gt'].isin([0, 1, 2])]\n",
        "\n",
        "    # convert data to NumPy data structures\n",
        "    x = df.iloc[:, 0:32].to_numpy()\n",
        "    y = df.iloc[:, 32].to_numpy().reshape(-1, 1)\n",
        "\n",
        "    # split data into training and test sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    # normalize data\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8PV4pEWhdSW"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "edt-phIAhe2m"
      },
      "outputs": [],
      "source": [
        "def gaussian_kernel(x, y, gamma):\n",
        "    '''\n",
        "    This function implements the Gaussian kernel.\n",
        "\n",
        "    For the implementation we just used the formula provided:\n",
        "    K(x, y) = exp(- gamma * ||x - y||^2)\n",
        "\n",
        "    We implement it in a vectorized way since we will need it to take both X\n",
        "    and Y as 2D arrays for the construction of the K matrix and one 1D and the\n",
        "    other 2D for the inference.\n",
        "    '''\n",
        "\n",
        "    # if x is 1D reshape to 2D\n",
        "    if x.ndim == 1:\n",
        "        x = x.reshape(-1, 1)\n",
        "\n",
        "    # if x is 1D reshape to 2D\n",
        "    if y.ndim == 1:\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "    # vectorized euclidean distances matrix\n",
        "    dists = np.sum((x[:, np.newaxis, :] - y[np.newaxis, :, :])**2, axis = -1)\n",
        "\n",
        "    # compute kernel\n",
        "    K = np.exp(- gamma * dists)\n",
        "\n",
        "    # if both x and y were single samples, return scalar\n",
        "    if K.size == 1:\n",
        "        return K.item()\n",
        "\n",
        "    # flatten if result is 1D array\n",
        "    elif K.ndim == 2 and (K.shape[0] == 1 or K.shape[1] == 1):\n",
        "        return K.flatten()\n",
        "\n",
        "    # else return result matrix\n",
        "    else:\n",
        "        return K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KaNvGMbhp-u"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OzXZrNghvz4"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rhh0CKV46isD"
      },
      "outputs": [],
      "source": [
        "class SVM():\n",
        "    '''\n",
        "    This class is an implementation of a Support Vector Machine.\n",
        "\n",
        "    The input to the class constructor are the kernel function, the parameter\n",
        "    of the kernel function and the value C, the upper bound of the constraints\n",
        "    of the dual problem.\n",
        "\n",
        "    The optimization procedure finds the support vectors and the bias while the\n",
        "    hyperparameters are to be found externally through cross-validation.\n",
        "\n",
        "    The training of the SVM is carried out as an optimization problem, we\n",
        "    express the objective function and the constraints of the dual problem as\n",
        "    a convex quadratic programming problem and solve it numerically using the\n",
        "    library CVXOPT.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, kernel_function, kernel_parameter, C, x, y):\n",
        "        # store the parameters of the model inside the object\n",
        "        self.kernel_function = globals().get(kernel_function)\n",
        "        self.kernel_parameter = kernel_parameter\n",
        "        self.C = C\n",
        "\n",
        "        # store the training data inside the object\n",
        "        self.x = x\n",
        "        self.L = x.shape[0]\n",
        "        self.y = y\n",
        "\n",
        "        # initial guess for lambda vector\n",
        "        self.lam = np.zeros((self.L))\n",
        "\n",
        "        # initialize K and Q matrices\n",
        "        self.K = self.build_K_matrix()\n",
        "        self.Q = self.build_Q_matrix()\n",
        "\n",
        "    def build_K_matrix(self):\n",
        "        '''\n",
        "        This function creates a matrix of size L x L such that:\n",
        "\n",
        "        K[i, j] = kernel_function(x_i, x_j)\n",
        "\n",
        "        This matrix is used to build the Q matrix but also to calculate the bias\n",
        "        at the end of the training.\n",
        "        '''\n",
        "\n",
        "        return self.kernel_function(self.x, self.x, self.kernel_parameter)\n",
        "\n",
        "    def build_Q_matrix(self):\n",
        "        \"\"\"\n",
        "        This function creates a matrix of size L x L such that:\n",
        "\n",
        "        Q[i, j] = y_i * y_i * kernel_function(x_i, x_j)\n",
        "\n",
        "        This is the matrix Q in the objective function of the SVM problem.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.K * np.outer(self.y, self.y)\n",
        "\n",
        "    def objective(self):\n",
        "        '''\n",
        "        This function calculates the objective function of the dual problem\n",
        "        using the current self.lam values. Objective function is:\n",
        "\n",
        "        (1/2) \\lambda^T Q \\lambda - e^T \\lambda\n",
        "\n",
        "        This function is not needed for the training or the inference but the\n",
        "        homework require us to report its value before and after the training\n",
        "        '''\n",
        "\n",
        "        return (1/2) * np.dot(np.dot(self.lam.T, self.Q), self.lam) - np.sum(self.lam)\n",
        "\n",
        "    def wks(self, x):\n",
        "        '''\n",
        "        This function implements what we call the \"weighted kernel sum\", we gave\n",
        "        it this name because it is the sum of the kernel function between the\n",
        "        input vector x and every support vector multiplied by the sign (class)\n",
        "        of the support vector and weighted by the corresponding lambda.\n",
        "\n",
        "        It implements the formula:\n",
        "\n",
        "        \\sum_{j \\in SV} \\lambda_j y_j K(x_j, x)\n",
        "\n",
        "        Where SV is the set of support vectors and x is the input.\n",
        "\n",
        "        The function has been vectorized to accept x as a 2D array of shape\n",
        "        (n_features, n_samples) to avoid the use of loops or list comprehensions.\n",
        "        '''\n",
        "\n",
        "        return np.sum(self.sv_lam.reshape(-1, 1) * self.sv_y.reshape(-1, 1) * self.kernel_function(self.sv_x, x, self.kernel_parameter), axis = 0)\n",
        "\n",
        "    def optimize(self):\n",
        "        '''\n",
        "        This function sets up the matrix components of the optimization problem\n",
        "        and solves it through the CVXOPT library.\n",
        "\n",
        "        Initialization of the components follow the reasonings made in the report\n",
        "        to translate the soft-margin SVM into the form required by CVXOPT.\n",
        "        '''\n",
        "\n",
        "        # initialize problem components\n",
        "\n",
        "        P = matrix(self.Q)\n",
        "        q = matrix(-np.ones((self.L, 1)).astype(float))\n",
        "\n",
        "        G = matrix(np.vstack((-np.eye(self.L), np.eye(self.L))).astype(float))\n",
        "        h = matrix(np.vstack((np.zeros((self.L, 1)), self.C * np.ones((self.L, 1)))).astype(float))\n",
        "\n",
        "        A = matrix(self.y.T.astype(float))\n",
        "        b = matrix(float(0))\n",
        "\n",
        "        # initial guess\n",
        "        x0 = matrix(self.lam)\n",
        "\n",
        "        # start measuring time\n",
        "        time_init = time.process_time()\n",
        "\n",
        "        # solve the convex optimization problem\n",
        "        sol = solvers.qp(P, q, G, h, A, b, initvals={'x': x0})\n",
        "\n",
        "        # stop measuring time and compute CPU time of optimization\n",
        "        time_end = time.process_time()\n",
        "        time_delta = time_end - time_init\n",
        "\n",
        "        return sol, time_delta\n",
        "\n",
        "    def build_svm_params(self):\n",
        "        '''\n",
        "        From the dual solution recovers the solution of the primal. The support\n",
        "        vectors are all the samples which lambda is greater than 0.\n",
        "        '''\n",
        "\n",
        "        # as per definition support vectors are all those non-zero lambdas\n",
        "\n",
        "        # we are using the mask only for computational efficiency, if the lambda\n",
        "        # is zero the component has no weight on the result but efficiency is lost\n",
        "        sv_mask = self.lam.flatten() > 0\n",
        "\n",
        "        # for inference we need only the x, y and lambda of support vectors\n",
        "        self.sv_x = self.x[sv_mask]\n",
        "        self.sv_y = self.y[sv_mask].flatten()\n",
        "        self.sv_lam = self.lam[sv_mask]\n",
        "\n",
        "        # calculate bias\n",
        "        # here we are using the formula of the bias provided by the lectures\n",
        "        # b^* = 1 / |SV| \\sum_{h \\in SV} (y^h - \\sum_{j = 1}^{l} \\lambda_j y_j K(x_j, x_h))\n",
        "        self.sv_bias = np.mean(self.sv_y - np.dot(self.K[np.where(sv_mask)[0]], self.lam.reshape(-1, 1) * self.y))\n",
        "\n",
        "    def train(self, verbose = False):\n",
        "        '''\n",
        "        From the dual solution recovers the solution of the primal computing\n",
        "\n",
        "        The support vectors are all the samples which lambda is greater than 0,\n",
        "        considering a value for tolerance.\n",
        "        '''\n",
        "\n",
        "        # solve optimization problem and get solution\n",
        "        sol, time_delta = self.optimize()\n",
        "\n",
        "        # update multipliers\n",
        "        self.lam = np.array(sol['x']).flatten()\n",
        "\n",
        "        # create parameters for inference\n",
        "        self.build_svm_params()\n",
        "\n",
        "        if verbose:\n",
        "            return sol['iterations'], time_delta\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    def inference(self, x):\n",
        "        '''\n",
        "        This function takes as input an unseen vector x and makes a prediction\n",
        "        of its class using the model built during the training.\n",
        "\n",
        "        The function has been vectorized to accept x as a 2D array of shape\n",
        "        (n_features, n_samples) to avoid the use of loops or list comprehensions.\n",
        "        '''\n",
        "\n",
        "        return np.sign(self.wks(x) + self.sv_bias).reshape(-1, 1)\n",
        "\n",
        "    def accuracy(self, x, y):\n",
        "        '''\n",
        "        This function calculates the accuracy of the model.\n",
        "\n",
        "        accuracy = correctly predicted / total\n",
        "        '''\n",
        "\n",
        "        return np.sum(self.inference(x) == y) / y.shape[0]\n",
        "\n",
        "    def confusion_matrix(self, x, y):\n",
        "        '''\n",
        "        This function calculates the confusion matrix from the provided data.\n",
        "        '''\n",
        "\n",
        "        # get subsets of samples\n",
        "        x_class1 = x[y.flatten() == 1]\n",
        "        x_class2 = x[y.flatten() == -1]\n",
        "\n",
        "        # get predictions for the subsets\n",
        "        y_class1 = self.inference(x_class1)\n",
        "        y_class2 = self.inference(x_class2)\n",
        "\n",
        "        # create empty confusion matrix\n",
        "        confusion_matrix = np.zeros((2, 2))\n",
        "\n",
        "        # fill confusion matrix\n",
        "        confusion_matrix[0, 0] = np.sum(y_class1 == 1)\n",
        "        confusion_matrix[0, 1] = np.sum(y_class1 == -1)\n",
        "\n",
        "        confusion_matrix[1, 0] = np.sum(y_class2 == 1)\n",
        "        confusion_matrix[1, 1] = np.sum(y_class2 == -1)\n",
        "\n",
        "        return confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7FXRcfhy3x"
      },
      "source": [
        "### Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TnzD5ilvh1sS"
      },
      "outputs": [],
      "source": [
        "def SVM_cross_validation(kernel_functions, kernel_parameters, C_values, k, show_progress = True):\n",
        "    '''\n",
        "    This function implements the cross-validation for the SVM hyperparameters.\n",
        "\n",
        "    It creates the list of all possible combination of hyperparameters and for\n",
        "    each combination trains k models using each time a different fold as\n",
        "    validation set.\n",
        "\n",
        "    Performances are measured and averaged across all folds and the best\n",
        "    combination of hyperparameters is returned, together with its performances.\n",
        "    '''\n",
        "\n",
        "    # load data question 2-3\n",
        "    x, _, y, _ = prepare_data_q23()\n",
        "\n",
        "    # generate all possible combinations of kernel structures, kernel parameters and C values\n",
        "    combinations = list(itertools.product(kernel_functions, kernel_parameters, C_values))\n",
        "\n",
        "    # split train data into folds\n",
        "    k_folds = KFold(n_splits = k)\n",
        "\n",
        "    # initialize variables to find best combination\n",
        "    best_val_accuracy = 0\n",
        "    best_train_accuracy = 0\n",
        "    best_combination = None\n",
        "\n",
        "    for combination in tqdm(combinations) if show_progress else combinations:\n",
        "        fold_val_accuracy = []\n",
        "        fold_train_accuracy = []\n",
        "\n",
        "        for train_index, val_index in k_folds.split(x):\n",
        "            # split data into train and validation sets\n",
        "            x_train = x[train_index]\n",
        "            y_train = y[train_index]\n",
        "\n",
        "            x_val = x[val_index]\n",
        "            y_val = y[val_index]\n",
        "\n",
        "            # initialize and train model\n",
        "            np.random.seed(42)\n",
        "            model = SVM(combination[0], combination[1], combination[2], x_train, y_train)\n",
        "            model.train()\n",
        "\n",
        "            # evaluate model\n",
        "            fold_val_accuracy.append(model.accuracy(x_val, y_val))\n",
        "            fold_train_accuracy.append(model.accuracy(x_train, y_train))\n",
        "\n",
        "        # compute combination accuracy as the mean of the MAPE for all folds\n",
        "        mean_val_accuracy = np.mean(fold_val_accuracy)\n",
        "        mean_train_accuracy = np.mean(fold_train_accuracy)\n",
        "\n",
        "        # update best combination if current combination is better\n",
        "        if mean_val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = mean_val_accuracy\n",
        "            best_train_accuracy = mean_train_accuracy\n",
        "            best_combination = combination\n",
        "\n",
        "    return best_combination, best_val_accuracy, best_train_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj28PmO-htU1"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MVP():\n",
        "    '''\n",
        "    This class is an implementation of a Support Vector Machine trained using\n",
        "    the Sequential Minimal Optimization (SMO) decomposition method where the\n",
        "    Working Set Selection Rule (WSSR) is the Most Violating Pair (MVP).\n",
        "\n",
        "    Until the optimality condition is satisfied we calculate the Most Violating\n",
        "    Pair, solve the subproblem analytically and use the result of the\n",
        "    subproblem to update the multipliers, the gradient and the bias.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, kernel_function, kernel_parameter, C, x, y, opt_tol = 1e-1):\n",
        "        # store model paramenters inside the object\n",
        "        self.kernel_function = globals().get(kernel_function)\n",
        "        self.kernel_parameter = kernel_parameter\n",
        "        self.C = C\n",
        "        self.opt_tol = opt_tol\n",
        "\n",
        "        # store training data\n",
        "        self.x = x\n",
        "        self.L = x.shape[0]\n",
        "        self.y = y\n",
        "\n",
        "        # initialize interaction matrix\n",
        "        # it is useful to calculate it once and store it as it will called\n",
        "        # multiple times throughout the training algorihtm\n",
        "        self.K = self.build_K_matrix()\n",
        "        self.Q = self.build_Q_matrix()\n",
        "\n",
        "        # initial guess for a vector\n",
        "        self.a = np.zeros((self.L))\n",
        "\n",
        "        # initialize gradients as vector of -1\n",
        "        self.gradient = - np.ones((self.L, 1))\n",
        "\n",
        "        # initialize bias\n",
        "        self.sv_bias = 0\n",
        "\n",
        "    def build_K_matrix(self):\n",
        "        '''\n",
        "        This function creates a matrix of size L x L such that:\n",
        "\n",
        "        K[i, j] = kernel_function(x_i, x_j)\n",
        "\n",
        "        This matrix is used to build the Q matrix but also to calculate the bias\n",
        "        at the end of the training. Here in the MVP algorithm it is also used\n",
        "        for solving the subproblems.\n",
        "        '''\n",
        "\n",
        "        return self.kernel_function(self.x, self.x, self.kernel_parameter)\n",
        "\n",
        "    def build_Q_matrix(self):\n",
        "        \"\"\"\n",
        "        This function creates a matrix of size L x L such that:\n",
        "\n",
        "        Q[i, j] = y_i * y_i * kernel_function(x_i, x_j)\n",
        "\n",
        "        This is the matrix Q in the objective function of the SVM problem.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.K * np.outer(self.y, self.y)\n",
        "\n",
        "    def objective(self):\n",
        "        '''\n",
        "        This function calculates the objective function of the dual problem\n",
        "        using the current self.a values.\n",
        "        '''\n",
        "\n",
        "        return (1/2) * np.dot(np.dot(self.a.T, self.Q), self.a) - np.sum(self.a)\n",
        "\n",
        "    def wks(self, x):\n",
        "        '''\n",
        "        This function implements what we call the \"weighted kernel sum\", we gave\n",
        "        it this name because it is the sum of the kernel function between the\n",
        "        input vector x and every support vector multiplied by the sign (class)\n",
        "        of the support vector and weighted by the corresponding lambda.\n",
        "\n",
        "        It implements the formula:\n",
        "\n",
        "        \\sum_{j \\in SV} \\lambda_j y_j K(x_j, x)\n",
        "\n",
        "        Where SV is the set of support vectors and x is the input.\n",
        "\n",
        "        The function has been vectorized to accept x as a 2D array of shape\n",
        "        (n_features, n_samples) to avoid the use of loops or list comprehensions.\n",
        "        '''\n",
        "\n",
        "        return np.sum(self.sv_lam.reshape(-1, 1) * self.sv_y.reshape(-1, 1) * self.kernel_function(self.sv_x, x, self.kernel_parameter), axis = 0)\n",
        "\n",
        "    def descent_direction(self, i):\n",
        "        '''\n",
        "        This function computes - \\nabla_i f(a) / y_i, which is needed to find\n",
        "        the Most Violating Pair.\n",
        "        '''\n",
        "\n",
        "        return - self.gradient[i] / self.y[i]\n",
        "\n",
        "    def find_R_S(self, tol = 1e-8):\n",
        "        '''\n",
        "        This function calculates the two sets R and S needed for finding the\n",
        "        Most Violating Pair and for the optimality check.\n",
        "\n",
        "        Here we are simply applying the definitions provided by the lectures\n",
        "        to create the sets.\n",
        "\n",
        "        Even though this is an analytical solution computer precision may still\n",
        "        provide small errors, therefore we need to add a tolerance to consider\n",
        "        a value as 0 or C.\n",
        "        '''\n",
        "\n",
        "        # {i = 1,..,l : a_i = 0 \\and y^i = -1}\n",
        "        L_minus = np.where((self.a <= 0 + tol) & (self.y.flatten() == -1))[0]\n",
        "\n",
        "        # {i = 1,..,l : a_i = 0 \\and y^i = 1}\n",
        "        L_plus = np.where((self.a <= 0 + tol) & (self.y.flatten() == 1))[0]\n",
        "\n",
        "        # {i = 1,..,l : a_i = C \\and y^i = -1}\n",
        "        U_minus = np.where((self.a >= self.C - tol) & (self.y.flatten() == -1))[0]\n",
        "\n",
        "        # {i = 1,..,l : a_i = C \\and y^i = 1}\n",
        "        U_plus = np.where((self.a >= self.C - tol) & (self.y.flatten() == 1))[0]\n",
        "\n",
        "        # {i : 0 < a_i < C}\n",
        "        middle = np.where((self.a > 0 + tol) & (self.a < self.C - tol))[0]\n",
        "\n",
        "        # L_plus U U_minus U middle\n",
        "        R = reduce(np.union1d, (L_plus, U_minus, middle))\n",
        "\n",
        "        # L_minus U U_plus U middle\n",
        "        S = reduce(np.union1d, (L_minus, U_plus, middle))\n",
        "\n",
        "        return R, S\n",
        "\n",
        "    def check_optimality(self, verbose = False):\n",
        "        '''\n",
        "        This function implements the check of the condition to stop the\n",
        "        optimization procedure. We say that a^k is optimal if:\n",
        "\n",
        "        max_{i \\in R(a^k)}{- \\nabla_i f(a^k) / y_i } <= min_{j \\in S(a^k)}{- \\nabla_j f(a^k) / y_j}\n",
        "\n",
        "        It is proved that SMO-MVP gets to an optimal solution in a finite number\n",
        "        of iterations however to avoid long computation times we add a tolerance\n",
        "        to the condition.\n",
        "        '''\n",
        "\n",
        "        R, S = self.find_R_S()\n",
        "\n",
        "        R_set = [self.descent_direction(i) for i in R]\n",
        "        S_set = [self.descent_direction(j) for j in S]\n",
        "\n",
        "        if len(R_set) == 0:\n",
        "            m = - float('inf')\n",
        "        else:\n",
        "            m = np.max(R_set)\n",
        "\n",
        "        if len(S_set) == 0:\n",
        "            M = float('inf')\n",
        "        else:\n",
        "            M = np.min(S_set)\n",
        "\n",
        "        if verbose:\n",
        "            return m, M\n",
        "        else:\n",
        "            if m <= (M + self.opt_tol):\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "    def difference(self):\n",
        "        \"\"\"\n",
        "        This function calculates the difference between m and M, which is needed\n",
        "        for the report. When the difference is negative the optimality condition\n",
        "        is satisfied.\n",
        "        \"\"\"\n",
        "\n",
        "        m, M = self.check_optimality(verbose = True)\n",
        "\n",
        "        return m - M - self.opt_tol\n",
        "\n",
        "    def find_mvp(self):\n",
        "        '''\n",
        "        This function finds the Most Violating Pair. It is the set of indices\n",
        "        {I, J} such that:\n",
        "\n",
        "        I(a^k) = argmax_{i \\in R(a^k)}{- \\nabla_i f(a^k) \\ y_i}\n",
        "        J(a^k) = argmin_{j \\in S(a^k)}{- \\nabla_j f(a^k) \\ y_j}\n",
        "        '''\n",
        "\n",
        "        R, S = self.find_R_S()\n",
        "\n",
        "        R_set = [(i, self.descent_direction(i)) for i in R]\n",
        "        S_set = [(j, self.descent_direction(j)) for j in S]\n",
        "\n",
        "        I = max(R_set, key = lambda z: z[1])[0]\n",
        "        J = min(S_set, key = lambda z: z[1])[0]\n",
        "\n",
        "        return I, J\n",
        "\n",
        "    def compute_E(self, i):\n",
        "        \"\"\"\n",
        "        This function computes the difference between the current value of the\n",
        "        prediction for the vector x_i before the sign function and the true\n",
        "        value of the prediction.\n",
        "        \"\"\"\n",
        "\n",
        "        return (np.dot(self.K[i], self.a.reshape(-1, 1) * self.y) + self.sv_bias - self.y[i])[0]\n",
        "\n",
        "    def solve_subproblem(self, i, j):\n",
        "        \"\"\"\n",
        "        Solves the quadratic programming subproblem of size 2. The algorithm\n",
        "        uses the transposition into code of what is described mathematically in\n",
        "        the 1998 paper by John C. Platt \"Sequential Minimal Optimization: A Fast\n",
        "        Algorithm for Training Support Vector Machines\", the paper that firstly\n",
        "        proposed the SMO algorithm. The paper is available here:\n",
        "\n",
        "        https://www.microsoft.com/en-us/research/uploads/prod/1998/04/sequential-minimal-optimization.pdf\n",
        "        \"\"\"\n",
        "\n",
        "        a_new = [None, None]\n",
        "\n",
        "        # get upper and lower bounds that satisfy the constraints\n",
        "        if self.y[i] != self.y[j]:\n",
        "            L = np.max([0, self.a[j] - self.a[i]])\n",
        "            H = np.min([self.C, self.C - self.a[i] + self.a[j]])\n",
        "\n",
        "        else:\n",
        "            L = np.max([0, self.a[i] + self.a[j] - self.C])\n",
        "            H = np.min([self.C, self.a[i] + self.a[j]])\n",
        "\n",
        "        # get E values for both i and j\n",
        "        Ei = self.compute_E(i)\n",
        "        Ej = self.compute_E(j)\n",
        "\n",
        "        # get new unconstrained optimizing value for aj\n",
        "        a_new[1] = self.a[j] + self.y[j] * (Ej - Ei) / (- self.K[i, i] - self.K[j, j] + 2 * self.K[i, j])\n",
        "\n",
        "        # clip aj to get optimizing value that enforces the constraints\n",
        "        a_new[1] = np.clip(a_new[1], L, H)[0]\n",
        "\n",
        "        # compute ai from aj, this is forced by equality constraint\n",
        "        a_new[0] = (self.a[i] + self.y[i] * self.y[j] * (self.a[j] - a_new[1]))[0]\n",
        "\n",
        "        # update the bias\n",
        "        # when both conditions are valid bi and bj are equal, when none is valid\n",
        "        # all thresholds are consistent and as done by the paper we select it\n",
        "        # halfway between the two\n",
        "\n",
        "        if a_new[0] > 0 and a_new[0] < self.C:\n",
        "            self.sv_bias = self.sv_bias - Ei - self.y[i] * (a_new[0] - self.a[i]) * self.K[i, i] - self.y[j] * (a_new[1] - self.a[j]) * self.K[i, j]\n",
        "\n",
        "        elif a_new[1] > 0 and a_new[1] < self.C:\n",
        "            self.sv_bias = self.sv_bias - Ej - self.y[i] * (a_new[0] - self.a[i]) * self.K[i, j] - self.y[j] * (a_new[1] - self.a[j]) * self.K[j, j]\n",
        "\n",
        "        else:\n",
        "            bi = self.sv_bias - Ei - self.y[i] * (a_new[0] - self.a[i]) * self.K[i, i] - self.y[j] * (a_new[1] - self.a[j]) * self.K[i, j]\n",
        "            bj = self.sv_bias - Ej - self.y[i] * (a_new[0] - self.a[i]) * self.K[i, j] - self.y[j] * (a_new[1] - self.a[j]) * self.K[j, j]\n",
        "            self.sv_bias = (bi + bj) / 2\n",
        "\n",
        "        return a_new\n",
        "\n",
        "    def optimization_step(self):\n",
        "        '''\n",
        "        Find the Most Violating Pair, compute a_i and a_j solving the quadratic\n",
        "        subproblem, update a, compute the new gradient vector.\n",
        "        '''\n",
        "\n",
        "        # find the most violating pair for the current a\n",
        "        i, j = self.find_mvp()\n",
        "\n",
        "        ai_old = self.a[i]\n",
        "        aj_old = self.a[j]\n",
        "\n",
        "        # start measuring time\n",
        "        time_init = time.process_time()\n",
        "\n",
        "        # solve the convex optimization problem finding the new values for\n",
        "        # a_i and a_j\n",
        "\n",
        "        self.a[i], self.a[j] = self.solve_subproblem(i, j)\n",
        "\n",
        "        # stop measuring time and compute CPU time of optimization\n",
        "        time_end = time.process_time()\n",
        "        time_delta = time_end - time_init\n",
        "\n",
        "        # compute and memorize the new gradient that will be needed for new\n",
        "        # iterations and optimality checks\n",
        "\n",
        "        self.gradient = self.gradient + self.Q[:,i].reshape(-1, 1) * (self.a[i] - ai_old) + self.Q[:,j].reshape(-1, 1) * (self.a[j] - aj_old)\n",
        "\n",
        "        return time_delta\n",
        "\n",
        "    def build_svm_params(self):\n",
        "        '''\n",
        "        From the dual solution recovers the solution of the primal. The support\n",
        "        vectors are all the samples which lambda is greater than 0.\n",
        "        '''\n",
        "\n",
        "        # as per definition support vectors are all those non-zero lambdas\n",
        "\n",
        "        # we are using the mask only for computational efficiency, if the lambda\n",
        "        # is zero the component has no weight on the result but efficiency is lost\n",
        "        sv_mask = self.a.flatten() > 0\n",
        "\n",
        "        # for inference we need only the x, y and lambda of support vectors\n",
        "        self.sv_x = self.x[sv_mask]\n",
        "        self.sv_y = self.y[sv_mask].flatten()\n",
        "        self.sv_lam = self.a[sv_mask]\n",
        "\n",
        "        # calculate bias\n",
        "        # here we are using the formula of the bias provided by the lectures\n",
        "        # b^* = 1 / |SV| \\sum_{h \\in SV} (y^h - \\sum_{j = 1}^{l} \\lambda_j y_j K(x_j, x_h))\n",
        "        self.sv_bias = np.mean(self.sv_y - np.dot(self.K[np.where(sv_mask)[0]], self.a.reshape(-1, 1) * self.y))\n",
        "\n",
        "    def train(self, verbose = False):\n",
        "        '''\n",
        "        From the dual solution recovers the solution of the primal finding\n",
        "        the parameters lambda and b.\n",
        "\n",
        "        The support vectors are all the lambda that are greater than 0,\n",
        "        considering a value for tolerance.\n",
        "        '''\n",
        "\n",
        "        # initialize iteration counters\n",
        "        iterations = 0\n",
        "        time_delta = 0\n",
        "\n",
        "        while not self.check_optimality():\n",
        "            # execute optimization step\n",
        "            current_time_delta = self.optimization_step()\n",
        "\n",
        "            # update counters\n",
        "            iterations += 1\n",
        "            time_delta += current_time_delta\n",
        "\n",
        "        self.build_svm_params()\n",
        "\n",
        "        if verbose:\n",
        "            return iterations, time_delta\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    def inference(self, x):\n",
        "        '''\n",
        "        This function takes as input an unseen vector x and makes a prediction\n",
        "        of its class using the model built during the training.\n",
        "\n",
        "        The function has been vectorized to accept x as a 2D array of shape\n",
        "        (n_features, n_samples) to avoid the use of loops or list comprehensions.\n",
        "        '''\n",
        "\n",
        "        return np.sign(self.wks(x) + self.sv_bias).reshape(-1, 1)\n",
        "\n",
        "    def accuracy(self, x, y):\n",
        "        '''\n",
        "        This function calculates the accuracy of the model.\n",
        "\n",
        "        accuracy = correctly predicted / total\n",
        "        '''\n",
        "\n",
        "        return np.sum(self.inference(x) == y) / y.shape[0]"
      ],
      "metadata": {
        "id": "8ayi_koZLLiB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCJltH8h4kd"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hv76rEHn9JiX"
      },
      "outputs": [],
      "source": [
        "class SVM_multiclass():\n",
        "    '''\n",
        "    This class is an implementation of a multiclass classification using\n",
        "    Support Vector Machines.\n",
        "\n",
        "    To implement the multiclass classification it has been used a One-vs-One\n",
        "    approach and the choosen classification is selected by majority voting.\n",
        "\n",
        "    To increase computation speed the SVM is trained using SMO-MVP\n",
        "    '''\n",
        "\n",
        "    def __init__(self, kernel_function, kernel_parameter, C, x, y):\n",
        "        # store model paramenters inside the object\n",
        "        self.kernel_function = kernel_function\n",
        "        self.kernel_parameter = kernel_parameter\n",
        "        self.C = C\n",
        "\n",
        "        # store training data\n",
        "        self.x = x\n",
        "        self.L = self.x.shape[0]\n",
        "        self.y = y\n",
        "\n",
        "        # set of classes the classificator has to deal with\n",
        "        self.classes = np.unique(y)\n",
        "\n",
        "    def train(self, verbose = False):\n",
        "        '''\n",
        "        This function implements the training for the multiclass classification.\n",
        "\n",
        "        For each combination of classes it is created, trained and stored a\n",
        "        Support Vector Machine.\n",
        "        '''\n",
        "\n",
        "        # initialize SVM list\n",
        "        self.SVMs = []\n",
        "\n",
        "        # iterations\n",
        "        iterations = 0\n",
        "        time_delta = 0\n",
        "\n",
        "        # cycles through each combination of classes\n",
        "        for class1, class2 in itertools.combinations(self.classes, 2):\n",
        "            # create mask for the data of the two selected ckasses\n",
        "            mask = np.logical_or(self.y == class1, self.y == class2).flatten()\n",
        "\n",
        "            # apply mask on train data\n",
        "            x_train = self.x[mask, :]\n",
        "            y_train = self.y[mask]\n",
        "\n",
        "            # convert classification value to -1 and +1 as SVM use this\n",
        "            # kind of classification\n",
        "            y_train[y_train == class1] = -1\n",
        "            y_train[y_train == class2] = 1\n",
        "\n",
        "            # create and train the SVM\n",
        "            model = MVP(self.kernel_function, self.kernel_parameter, self.C, x_train, y_train)\n",
        "            current_iterations, current_time_delta = model.train(verbose = True)\n",
        "\n",
        "            # update iterations\n",
        "            iterations += current_iterations\n",
        "            time_delta += current_time_delta\n",
        "\n",
        "            # insert trained SVM into the list together with identifiers of the\n",
        "            # two classes the SVM classifies\n",
        "            self.SVMs.append(((class1, class2), model))\n",
        "\n",
        "        if verbose:\n",
        "            return iterations, time_delta\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    def inference(self, x):\n",
        "        '''\n",
        "        This function implements the inference for the multiclass classification.\n",
        "\n",
        "        The function has been vectorized to accept x as a 2D array of shape\n",
        "        (n_features, n_samples) to avoid the use of loops or list comprehensions.\n",
        "\n",
        "        It creates a matrix where votes for each sample for each class are\n",
        "        stored, it populates the matrix using the inference on every SVM inside\n",
        "        the model and then it returns the class with the most votes.\n",
        "        '''\n",
        "\n",
        "        # initialize votes matrix\n",
        "        votes = np.zeros((x.shape[0], len(self.classes)))\n",
        "\n",
        "        # iterate through all the trained SVMs\n",
        "        for (class1, class2), SVM in self.SVMs:\n",
        "\n",
        "            # get predictions from the SVM\n",
        "            predictions = SVM.inference(x)\n",
        "\n",
        "            # convert prediction to masks\n",
        "            predictions_class1 = (predictions == -1)\n",
        "            predictions_class2 = (predictions == 1)\n",
        "\n",
        "            # update votes matrix\n",
        "            votes[:, np.where(self.classes == class1)[0]] += predictions_class1\n",
        "            votes[:, np.where(self.classes == class2)[0]] += predictions_class2\n",
        "\n",
        "        # return predictions\n",
        "        return self.classes[np.argmax(votes, axis=1)].reshape(-1, 1)\n",
        "\n",
        "    def accuracy(self, x, y):\n",
        "        return round(np.sum(self.inference(x) == y) / y.shape[0], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOGG2pwaisoK"
      },
      "source": [
        "## Required prints, values and plots to compile the report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aVmwugsQivea"
      },
      "outputs": [],
      "source": [
        "def SVM_print(kernel_function, kernel_parameter, C):\n",
        "\n",
        "    print(f\"Setting values of the hyperparameters:\")\n",
        "    print(f\"\\tC = {C}\")\n",
        "    print(f\"\\tActivation function = {kernel_function}\")\n",
        "    print(f\"\\tgamma = {kernel_parameter}\")\n",
        "\n",
        "    # load data question 2-3\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q23()\n",
        "\n",
        "    # initialize and train model\n",
        "    np.random.seed(42)\n",
        "    model = SVM(kernel_function, kernel_parameter, C, x_train, y_train)\n",
        "    iterations, time_delta = model.train(verbose = True)\n",
        "    objective = model.objective()\n",
        "\n",
        "    print(f\"Classification rate on the training set (% instances correctly classified): {model.accuracy(x_train, y_train)}\")\n",
        "    print(f\"Classification rate on the test set (% instances correctly classified): {model.accuracy(x_test, y_test)}\")\n",
        "\n",
        "    # The confusion matrix\n",
        "    print(f\"The confusion matrix:\")\n",
        "    print(f\"{model.confusion_matrix(x_test, y_test)}\")\n",
        "\n",
        "    print(f\"Time necessary for the optimization procedure: {time_delta} seconds\")\n",
        "    print(f\"Number of optimization iterations: {iterations}\")\n",
        "\n",
        "    # Final difference between m(λ) and M(λ) (only for question 3)\n",
        "    np.random.seed(42)\n",
        "    model = MVP(kernel_function, kernel_parameter, C, x_train, y_train)\n",
        "    model.train()\n",
        "    difference = model.difference()\n",
        "    print(f\"Final difference between m(λ) and M(λ) (question 3): {difference}\")\n",
        "\n",
        "    print(f\"Final value of the objective function of the dual SVM problem: {objective}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jXq3DQ_8ttuu"
      },
      "outputs": [],
      "source": [
        "def SVM_report(kernel_function, kernel_parameter, C):\n",
        "\n",
        "    # load data question 2-3\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q23()\n",
        "\n",
        "    # initialize SVM\n",
        "    np.random.seed(42)\n",
        "    model = SVM(kernel_function, kernel_parameter, C, x_train, y_train)\n",
        "\n",
        "    # get initial objective\n",
        "    initial_objective = model.objective()\n",
        "\n",
        "    # train model and get values to report\n",
        "    iterations, time_delta = model.train(verbose = True)\n",
        "\n",
        "    final_objective = model.objective()\n",
        "\n",
        "    # the final setting for the hyperparameter C (upper bound of the constraints\n",
        "    # of the dual problem) and of the hyperparameter of the kernel chosen; how\n",
        "    # you have chosen them and if you could identify values that highlight\n",
        "    # over/underfitting;\n",
        "    print(f\"Setting values of the hyperparameters:\")\n",
        "    print(f\"\\tKernel function = {kernel_function}\")\n",
        "    print(f\"\\tC = {C}\")\n",
        "    print(f\"\\tgamma = {kernel_parameter}\")\n",
        "    # identify values that highlight overfitting/underfitting?\n",
        "\n",
        "    # which optimization routine you use for solving the quadratic minimization\n",
        "    # problem and the setting of its parameters, if any (write DEFAULT if you\n",
        "    # have not changed them);\n",
        "    print(f\"Optimization routine used: cvxopt.solvers.qp()\")\n",
        "    print(f\"Optimization routine parameters:\")\n",
        "\n",
        "    # write the value of the accuracy on the training, validation and test set\n",
        "    np.random.seed(42)\n",
        "    _, best_val_accuracy, _ = SVM_cross_validation([kernel_function], [kernel_parameter], [C], k = 5, show_progress = False)\n",
        "    print(f\"Accuracy of SVM on train set: {model.accuracy(x_train, y_train)}\")\n",
        "    print(f\"Accuracy of SVM on validation set: {best_val_accuracy}\")\n",
        "    print(f\"Accuracy of SVM on test set: {model.accuracy(x_test, y_test)}\")\n",
        "\n",
        "    # report the initial and final value of the objective function of the dual\n",
        "    # problem and the number of iterations\n",
        "    print(f\"Initial value of the objective function of the dual SVM problem: {initial_objective}\")\n",
        "    print(f\"Final value of the objective function of the dual SVM problem: {final_objective}\")\n",
        "    print(f\"Number of optimization iterations: {iterations}\")\n",
        "    print(f\"Time necessary for the optimization procedure: {time_delta} seconds\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # initialize MVP SVM\n",
        "    np.random.seed(42)\n",
        "    model = MVP(kernel_function, kernel_parameter, C, x_train, y_train)\n",
        "\n",
        "    # get initial objective\n",
        "    initial_objective = model.objective()\n",
        "\n",
        "    # train model\n",
        "    iterations, time_delta = model.train(verbose = True)\n",
        "\n",
        "    # get final objective\n",
        "    final_objective = model.objective()\n",
        "\n",
        "    # write the value of the accuracy on the training and test set\n",
        "    print(f\"Accuracy of MVP SVM on train set: {model.accuracy(x_train, y_train)}\")\n",
        "    print(f\"Accuracy of MVP SVM on test set: {model.accuracy(x_test, y_test)}\")\n",
        "\n",
        "    # report the initial and final value of the objective function of the dual\n",
        "    # problem and the number of iterations\n",
        "    print(f\"Initial value of the objective function of the MVP problem: {initial_objective}\")\n",
        "    print(f\"Final value of the objective function of the MVP problem: {final_objective}\")\n",
        "    print(f\"Number of iterations: {iterations}\")\n",
        "    print(f\"Time necessary for the optimization procedure: {time_delta} seconds\")\n",
        "\n",
        "    # TABLE\n",
        "    print(f\"Kernel function: {kernel_function}\")\n",
        "    print(f\"C: {C}\")\n",
        "    print(f\"Kernel parameter: {kernel_parameter}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # load data question 4\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q4()\n",
        "\n",
        "    # initialize multiclass SVM\n",
        "    np.random.seed(42)\n",
        "    model = SVM_multiclass(kernel_function, kernel_parameter, C, x_train, y_train)\n",
        "\n",
        "    # train model\n",
        "    iterations, time_delta = model.train(verbose = True)\n",
        "\n",
        "    print(f\"Accuracy of multiclass SVM on train set: {model.accuracy(x_train, y_train)}\")\n",
        "    print(f\"Accuracy of multiclass SVM on test set: {model.accuracy(x_test, y_test)}\")\n",
        "\n",
        "    print(f\"Number: {iterations}\")\n",
        "    print(f\"Time necessary for the optimization procedure: {time_delta} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4qV52pKtzypT"
      },
      "outputs": [],
      "source": [
        "def plot_varying_gamma(kernel_function, gammas, C):\n",
        "    \"\"\"\n",
        "    Trains multiple models with fixed C and varying gamma, then plot the training\n",
        "    and test accuracy to show overfitting and underfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    # load data question 2-3\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q23()\n",
        "\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    for gamma in tqdm(gammas):\n",
        "        # initialize and train model\n",
        "        model = SVM(kernel_function, gamma, C, x_train, y_train)\n",
        "        model.train()\n",
        "\n",
        "        # compute accuracies\n",
        "        train_accs.append(model.accuracy(x_train, y_train))\n",
        "        test_accs.append(model.accuracy(x_test, y_test))\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(gammas, train_accs, label='Train Accuracy', marker='o')\n",
        "    plt.plot(gammas, test_accs, label='Test Accuracy', marker='o')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Gamma')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Train and Test Accuracy vs. Gamma (Fixed C)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def ploy_varying_C(kernel_function, gamma, C_values):\n",
        "    \"\"\"\n",
        "    Trains multiple models with fixed gamma and varying C, then plots the training\n",
        "    and test accuracy to show overfitting and underfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    # load data question 2-3\n",
        "    x_train, x_test, y_train, y_test = prepare_data_q23()\n",
        "\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    for C in tqdm(C_values):\n",
        "        # initialize and train model\n",
        "        model = SVM(kernel_function, gamma, C, x_train, y_train)\n",
        "        model.train()\n",
        "\n",
        "        # compute accuracies\n",
        "        train_accs.append(model.accuracy(x_train, y_train))\n",
        "        test_accs.append(model.accuracy(x_test, y_test))\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(C_values, train_accs, label='Train Accuracy', marker='o')\n",
        "    plt.plot(C_values, test_accs, label='Test Accuracy', marker='o')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('C')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Train and Test Accuracy vs. C (Fixed Gamma)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAvwdEy-9GVK"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Ytx6sNpusAc"
      },
      "outputs": [],
      "source": [
        "# set solver settings\n",
        "solvers.options[\"show_progress\"] = False\n",
        "\n",
        "# set mode to hide all non-required prints\n",
        "hide_non_required_prints = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup cross-validation space\n",
        "kernel_functions = [\"gaussian_kernel\"]\n",
        "kernel_parameters = np.logspace(-8, 4, 13)\n",
        "C_values = np.logspace(-5, 3, 9)\n",
        "\n",
        "# execute cross-validation\n",
        "best_hyperparams, best_val_accuracy, best_train_accuracy = SVM_cross_validation(kernel_functions, kernel_parameters, C_values, k = 5)\n",
        "\n",
        "print(f\"\\nBest hyperparameters: {best_hyperparams}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuvmMjozFt95",
        "outputId": "beed6e5f-12ec-48be-ac3a-de2c14d2b669"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [09:49<00:00,  5.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best hyperparameters: ('gaussian_kernel', 0.1, 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of train and test accuracy vs C with fixed gamma\n",
        "\n",
        "# non-required and hidden\n",
        "if not hide_non_required_prints:\n",
        "    ploy_varying_C(kernel_function = \"gaussian_kernel\",\n",
        "                   gamma = 0.1,\n",
        "                   C_values = np.logspace(-5, 3, 5000))"
      ],
      "metadata": {
        "id": "_WGEWdzbHDiR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of train and test accuracy vs gamma with fixed C\n",
        "\n",
        "# non-required and hidden\n",
        "if not hide_non_required_prints:\n",
        "    plot_varying_gamma(kernel_function = \"gaussian_kernel\",\n",
        "                       C = 1.0,\n",
        "                       gammas = np.logspace(-8, 4, 5000))"
      ],
      "metadata": {
        "id": "ZD6jbRgLm_h2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IdTCAXjW9QVM"
      },
      "outputs": [],
      "source": [
        "# print values that are needed to compile the report but are not required\n",
        "# to be printed on the notebook\n",
        "\n",
        "# non-required and hidden\n",
        "if not hide_non_required_prints:\n",
        "    SVM_report(kernel_function = \"gaussian_kernel\",\n",
        "               kernel_parameter = 0.1,\n",
        "               C = 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print values that are required to be printed on the notebook\n",
        "\n",
        "SVM_print(kernel_function = \"gaussian_kernel\",\n",
        "          kernel_parameter = 0.1,\n",
        "          C = 1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GIZvyid_F5j",
        "outputId": "b4ca5d9c-da8e-4ddd-8f5a-906f8828492a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting values of the hyperparameters:\n",
            "\tC = 1.0\n",
            "\tActivation function = gaussian_kernel\n",
            "\tgamma = 0.1\n",
            "Classification rate on the training set (% instances correctly classified): 0.9175\n",
            "Classification rate on the test set (% instances correctly classified): 0.92\n",
            "The confusion matrix:\n",
            "[[94. 10.]\n",
            " [ 6. 90.]]\n",
            "Time necessary for the optimization procedure: 1.5889891650000436 seconds\n",
            "Number of optimization iterations: 13\n",
            "Final difference between m(λ) and M(λ) (question 3): -0.009602091335748075\n",
            "Final value of the objective function of the dual SVM problem: -141.57524979235575\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}